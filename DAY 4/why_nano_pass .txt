Here are detailed notes on why many small passes (as in the Nanopass / micropass approach) are considered better (and what trade-offs are involved). These are drawn from Flatt & Felleisen, “The NanoPass Framework”, Keep & Dybvig et al., and related talks. If you like, I can also sketch a summary in bullet-form for quick reference.

What is the Nanopass (Many-Small-Pass) Approach, in a Nutshell

Before going into “why it’s good”, let me restate the idea clearly:

A compiler is written not as a few large, “monolithic” passes each doing many things, but rather as many fine-grained passes, each pass having a single responsibility (or at least a narrowly defined transformation).

Between passes, there are intermediate languages (IRs) with well-defined grammars. Each pass typically transforms from one IR to another.

The “Nanopass framework” provides infrastructure (in Racket / Scheme, in some tutorials, and later commercial use) to make writing these passes and defining the IRs easier, suppress boilerplate, checking, etc. 
ACM Digital Library
+4
Manuals+
+4
andykeep.com
+4

Why Many Small Passes are Better

Here are the main advantages, with detail:

Modularity and Composability

Each pass has one job / transformation. That means each is easier to reason about, test, and verify in isolation.

If you want to add a new optimization (or change some transformation), you just insert or modify a small pass, rather than changing a large pass that does many things. This reduces coupling. 
ACM Digital Library
+1

Maintainability

Smaller passes reduce complexity: less code to inspect, fewer interactions per pass. The mental burden is smaller.

Bugs/localizing errors is easier, because the scope of what a given pass can do is small. If something goes wrong, you can often pinpoint which pass is misbehaving. 
Manuals+
+2
ACM Digital Library
+2

Understandability and Pedagogical Value

The papers stress that for teaching compiler construction, nanopass compilers are much more transparent for students. Each pass is small and comprehensible. 
ACM Digital Library
+1

When passes correspond closely to the logical steps one learns (e.g. syntactic sugar removal, variable renaming, closure conversion, etc.), there is a clearer mapping between theory and implementation. This helps learning. 
Manuals+
+1

Flexibility and Extensibility

Because passes are independent (or loosely coupled), you can reorder, insert, or remove passes with less risk. If you want a new feature or an experimental optimization, you can often slot in a new pass. 
Manuals+
+1

Intermediate languages let you gradually transform from high-level to low-level. IRs can capture invariants, constraints, etc. As passes decrease abstraction gradually, each pass can assume something about the input IR (e.g. that certain constructs are gone). This allows optimizations that expect simpler IRs. 
andykeep.com
+1

Correctness and Safety

Formalizing the grammars of IRs helps catch errors early: the compiler can enforce that output of pass P matches the expected IR grammar. This prevents certain classes of bugs. 
guenchi.github.io
+2
andykeep.com
+2

Because passes are small, verifying correctness (formal or empirical) is easier. Easier to write tests for each pass. Also, invariants are more local.

Optimization and Performance Gains (Sometimes)

Although intuitively more passes might mean more overhead, nanopass results (in a commercial setting) show that the code produced can be faster, because modular passes enable better optimizations when abstractions are peeled away cleanly. For example, in “A Nanopass Framework for Commercial Compiler Development”, the nanopass variant of Chez Scheme produced code that was 15-27% faster on average. 
andykeep.com
+1

Also, some passes can simplify the IR so later passes or back‐ends can work more efficiently.

Better Tooling, Debugging, & Testing

You can inspect the output of each pass; compare IRs, test equivalence, run invariants. This is very helpful for debugging. 
Manuals+
+1

Easier to do regression tests per pass. Each pass becomes a unit you can test.

Clear Separation of Concerns

Different concerns (syntax sugar, control flow, optimizations, lowering, register allocation, etc.) are cleanly separated. This improves clarity and avoids having code in one pass interfere with another’s responsibility.

Scalability of Compiler Development & Feature Addition

As languages evolve, or one wants to target multiple back‐ends, or support many architectures / platform specifics, having many small passes makes supporting variants easier: you can add passes specific to certain backends, and leave others unchanged.

What Are the Costs / Trade-Offs

It’s not all upside. The framework and the papers also discuss drawbacks and how they mitigate them.

Overhead / Performance Costs

More passes means more traversal of AST / IRs. Each pass may walk over large parts of the program, possibly rebuilding data structures. If done naively, significant runtime cost. 
ACM Digital Library
+1

Also, boilerplate: passes often have to ignore many parts of the IR that are unchanged; traversals have to visit all nodes, even if only a few are changed. This can add code overhead and reduce readability if not handled well. 
Manuals+
+2
guenchi.github.io
+2

Complexity of Managing IRs & Grammar Versions

Since there are many intermediate languages, one must define their grammars, maintain them, and ensure passes produce outputs matching them. That’s extra effort. If grammars are informal, mismatches cause errors. 
guenchi.github.io
+1

Ensuring consistency among passes: if one pass assumes something about the IR that isn’t guaranteed, or another pass violates invariants, subtle bugs can crop up.

Tooling / Boilerplate Avoidance Needed

Without a framework to help, a lot of boilerplate is repeated: tree traversals, matching, reconstructing nodes, etc. That can obscure the core logic of passes. The Nanopass framework addresses this by automatically generating the necessary traversal machinery, grammar checking, etc. 
Manuals+
+1

Compilation Time Might Increase

More passes generally mean more time in pass startup, IR transformation, etc. The papers show that even with 50+ nanopasses replacing ~10 coarse passes, the resultant compilation times were within a constant factor (sometimes up to ≈2×) of original. So while slower, the overhead was acceptable given the gains. 
andykeep.com
+1

Cognitive Overhead of Many IRs

Having many IRs means keeping track of what invariants / constructs exist at each IR level. For developers, understanding the transformations across many layers can be challenging.

Risk of Over-fragmentation

If you go too fine-grained, then each pass may do so little, and the overhead of setting up / tearing down passes might dominate. Also, debugging can become harder if one has to step through many tiny passes to figure out where something changed.

How the Nanopass Framework Mitigates Costs

Some of the “why it works in practice” is in how the framework is designed to reduce drawbacks:

Define-language constructs to formally declare grammars of IRs so the system can validate that outputs of passes are well-formed. 
guenchi.github.io
+1

Infrastructure for writing passes in a succinct way, so that only the relevant part of a node needs to be specified (pattern matching with default behavior for other nodes). Reduces boilerplate. 
Manuals+
+1

Facilities for automatically generating traversal functions, translation between similar IRs etc. 
guenchi.github.io
+1

Tools for pruning unused parts of grammars, removing dead code, etc. 
guenchi.github.io
+1

Concrete Examples & Evidence

Here are some empirical / concrete points from the papers:

In “A nanopass infrastructure for compiler education” the authors show that broken or incorrect transformations can be isolated more quickly when passes are small, and tests / inspections are easier. 
ACM Digital Library
+1

In the commercial use (Chez Scheme reimplementation), they replaced ~5 large passes by over 50 nanopasses, yet compilation time only grew modestly. And generated code was even faster due to cleaner lower‐level IR and refined optimizations. 
andykeep.com
+1

Guiding Principles / When to Favor Many Small Passes

From the papers and talks, here are some design heuristics: when many small passes are especially advantageous, and when maybe less so.

When many small passes are good	When monolithic passes might be better / acceptable
You want extensibility, experimentation, many variants.	You have very strict performance constraints; overhead of passes must be minimized.
You want clear correctness, formal guarantees, or strong testability.	The transformations are tightly interdependent, and separating them is hard without duplicating work.
Language features/transformations are incremental/easy to decompose.	Transformations are complex, interwoven, and thinking in large blocks is more natural.
Teaching / prototyping / research.	Production setting where every millisecond of compile time matters (but even then, trade-offs may favor nanopass with optimizations).
My Reflections / Why Many Small Passes Are a Powerful Paradigm

They encourage clean design and force you to think in terms of composable transformations. This tends to produce more maintainable, flexible compilers.

They make it easier for teams: different people can work on different passes, understand only those parts.

The concept is related to separation of concerns, single responsibility principle, modular design in software engineering. Nanopass is a domain-specific instantiation of those principles for compilers.